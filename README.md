

#### References:
* bert https://arxiv.org/pdf/1810.04805.pdf
* attention is all you need https://arxiv.org/pdf/1706.03762.pdf
* https://huggingface.co/transformers/

#### Attempts on BERT nmt
* https://slator.com/machine-translation/does-googles-bert-matter-in-machine-translation/
* https://arxiv.org/abs/1909.12744
* https://arxiv.org/abs/1905.02450

#### Unsupervised NMT
* https://www.aclweb.org/anthology/P19-1019.pdf
* https://arxiv.org/pdf/1804.07755.pdf
* https://arxiv.org/pdf/2001.08210.pdf


* обучаем по словарю
* scaling up transformers like efficient nets?
* Recurrent Bert разное количество слоев

Adversarial NMT -- может быть хорошо применено к прогрессивному переводу
* https://arxiv.org/pdf/1905.11946v5.pdf