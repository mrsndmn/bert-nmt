schema: '2.0'
stages:
  train_tokenizer:
    cmd: python train_tokenizer.py --tokenizer-out .data/wmt20.bpetokenizer --dataset
      wmt20_mlqe_task1 --languages ru-en
    deps:
    - path: train_tokenizer.py
      md5: 22b178f99b49144a8d01e671edb8a968
      size: 1466
    outs:
    - path: .data/wmt20.bpetokenizer
      md5: 6283b43a10006105fa6d69735b91c78f
      size: 1030954
  echo_bert_inv_tokens:
    cmd: python train_bert.py --name echo_bert_inv_tokens --tokenizer .data/wmt20.bpetokenizer
      --dataset wmt20_mlqe_task1 --languages ru-en --batch_size 10 --noam_opt_warmup_steps
      1000 --gpus 1 --limit_val_batches=15 --max_epochs 15 --early_stopping_patience
      1000 --noam_scaler=0.3 --emb_norm_reg 0.0001
    deps:
    - path: .data/wmt20.bpetokenizer
      md5: e19dfb66cba6fd209f59ea9dccf70214
      size: 898442
    - path: train_bert.py
      md5: 5ee275616e9936b5b8ce9949d86537a9
      size: 13414
    - path: train_tokenizer.py
      md5: f744e0d25ea1b92a4457d49bdea19fc4
      size: 1334
    outs:
    - path: lightning_logs/echo_bert_inv_tokens
      md5: 5b0f0528d2f4732454c285080c3e8dbc.dir
      size: 113043107
      nfiles: 3
  translate_bert_inv_tokens:
    cmd: python train_bert.py --name translate_bert_inv_tokens --tokenizer .data/wmt20.bpetokenizer
      --dataset wmt20_mlqe_task1 --languages ru-en --batch_size 10 --noam_opt_warmup_steps
      1000 --gpus 1 --limit_val_batches=15 --max_epochs 200 --early_stopping_patience
      1000 --noam_scaler=1 --emb_norm_reg 0.0001
    deps:
    - path: .data/wmt20.bpetokenizer
      md5: ead1ef249b71bc5582bae2de9509a790
      size: 898544
    - path: train_bert.py
      md5: bfb4ec10cb5eec51f2883be74f4ee9fe
      size: 15383
    - path: train_tokenizer.py
      md5: ba8acf6440d424120b8cc0972f1e85d8
      size: 1347
    outs:
    - path: lightning_logs/translate_bert_inv_tokens
      md5: 9f5aaecc71f8820cb0fb59a3958aa393.dir
      size: 151699755
      nfiles: 3
